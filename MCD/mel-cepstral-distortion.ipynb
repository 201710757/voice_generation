{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel-Cepstral Distortion\n",
    "\n",
    "This notebook will show you how to calculate the Mel-cepstral distortion (MCD) of target reference and converterted synthesised wavs that are not time aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import glob\n",
    "import librosa\n",
    "import pyworld\n",
    "import pysptk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Functions and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav(wav_file, sr):  # wav file -> audio time series numpy array\n",
    "    \"\"\"\n",
    "    Load a wav file with librosa.\n",
    "    :param wav_file: path to wav file\n",
    "    :param sr: sampling rate\n",
    "    :return: audio time series numpy array\n",
    "    \"\"\"\n",
    "    wav, _ = librosa.load(wav_file, sr=sr, mono=True)\n",
    "\n",
    "    return wav\n",
    "\n",
    "\n",
    "def log_spec_dB_dist(x, y):\n",
    "    log_spec_dB_const = 10.0 / math.log(10.0) * math.sqrt(2.0)\n",
    "    diff = x - y\n",
    "    \n",
    "    return log_spec_dB_const * math.sqrt(np.inner(diff, diff))\n",
    "\n",
    "\n",
    "SAMPLING_RATE = 22050\n",
    "FRAME_PERIOD = 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 The Data\n",
    "\n",
    "The data used in this example is from LJSpeech and KSS using 99 and 78 sentences each.<br/>\n",
    "LJSpeech was synthesized with [DeepVoice3](https://github.com/generative-model/voice_generation/tree/main/deepvoice3_pytorch) and KSS was synthesized with [Tacotron2](https://github.com/generative-model/voice_generation/tree/main/Tacotron2-Wavenet-Korean-TTS)\n",
    " * LJSpeech (en): https://keithito.com/LJ-Speech-Dataset/\n",
    " * KSS (kr) : https://www.kaggle.com/bryanpark/korean-single-speaker-speech-dataset\n",
    "\n",
    "```\n",
    "official_dir/wavs\n",
    "    |\n",
    "    |- audio_origin\n",
    "        |- <TRG_ID>.wav\n",
    "        |- ...\n",
    "    |- audio_syn\n",
    "        |- syn_<TRG_ID>.wav\n",
    "        |- ...\n",
    "```\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Extract Acoustic Features\n",
    "\n",
    "From the StarGAN-VC2 paper:\n",
    "\n",
    "> Each speaker has sets of 81 and 35 sentences for training and evaluation, respectively. The recordings were downsampled to 22.05 kHz for this challenge. We extracted 34 Mel-cepstral coefficients (MCEPs), logarithmic fundamental frequency (log F0), and aperiodicities (APs) every 5 ms by using the WORLD analyzer [52].\n",
    "\n",
    "* MCD: 6.9 +- .08 dB for StarGAN-VC\n",
    "* MCD: 6.9 +- .07 dB for StarGAN-VC2\n",
    "* Link: http://www.kecl.ntt.co.jp/people/kaneko.takuhiro/projects/stargan-vc2/index.html\n",
    "\n",
    "Extract features using SPTK Mel-cepstral Analysis of speech (more info: http://sp-tk.sourceforge.net/)\n",
    "\n",
    "This notebook uses python wrappers of the WORLD vocoder and SPTK to extract acoustic features:\n",
    "\n",
    "* To get the spectral envelope: `pyworld` https://github.com/JeremyCCHsu/Python-Wrapper-for-World-Vocoder \n",
    "* To extract MCEP features: `pysptk` https://github.com/r9y9/pysptk  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav2mcep_numpy(wavfile, target_directory, alpha=0.65, fft_size=512, mcep_size=34): # .wav -> Mel-cepstral(.npy)\n",
    "    # make relevant directories\n",
    "    if not os.path.exists(target_directory):\n",
    "        os.makedirs(target_directory)\n",
    "\n",
    "    loaded_wav = load_wav(wavfile, sr=SAMPLING_RATE)\n",
    "\n",
    "    # Use WORLD vocoder to spectral envelope\n",
    "    _, sp, _ = pyworld.wav2world(loaded_wav.astype(np.double), fs=SAMPLING_RATE,\n",
    "                                   frame_period=FRAME_PERIOD, fft_size=fft_size)\n",
    "\n",
    "    # Extract MCEP features\n",
    "    mgc = pysptk.sptk.mcep(sp, order=mcep_size, alpha=alpha, maxiter=0,\n",
    "                           etype=1, eps=1.0E-8, min_det=0.0, itype=3)\n",
    "\n",
    "    fname = os.path.basename(wavfile).split('.')[0]\n",
    "    np.save(os.path.join(target_directory, fname + '.npy'),\n",
    "            mgc,\n",
    "            allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha = 0.65  # commonly used at 22050 Hz\n",
    "fft_size = 512\n",
    "mcep_size = 34\n",
    "\n",
    "\n",
    "deep_origin_wavs = glob.glob('./audio_deep/wavs/audio_origin/*') # wavs/audio_origin/ 안 모든 wav files\n",
    "deep_origin_mcep_dir = './audio_deep/mceps_numpy/audio_origin' # mceps_numpy/audio_origin 경로 \n",
    "deep_syn_wavs = glob.glob('./audio_deep/wavs/audio_syn/*')\n",
    "deep_syn_mcep_dir = './audio_deep/mceps_numpy/audio_syn'\n",
    "\n",
    "taco_origin_wavs = glob.glob('./audio_taco/wavs/audio_origin/*') \n",
    "taco_origin_mcep_dir = './audio_taco/mceps_numpy/audio_origin' \n",
    "taco_syn_wavs = glob.glob('./audio_taco/wavs/audio_syn/*')\n",
    "taco_syn_mcep_dir = './audio_taco/mceps_numpy/audio_syn'\n",
    "\n",
    "\n",
    "for wav in deep_origin_wavs: # file 내의 모든 .wav -> mcep_numpy\n",
    "    wav2mcep_numpy(wav, deep_origin_mcep_dir, fft_size=fft_size, mcep_size=mcep_size)\n",
    "\n",
    "for wav in deep_syn_wavs:\n",
    "    wav2mcep_numpy(wav, deep_syn_mcep_dir, fft_size=fft_size, mcep_size=mcep_size)\n",
    "\n",
    "for wav in taco_origin_wavs:\n",
    "    wav2mcep_numpy(wav, taco_origin_mcep_dir, fft_size=fft_size, mcep_size=mcep_size)\n",
    "\n",
    "for wav in taco_syn_wavs:\n",
    "    wav2mcep_numpy(wav, taco_syn_mcep_dir, fft_size=fft_size, mcep_size=mcep_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Calculate MCD\n",
    "\n",
    "* Here I use librosa to calculate dynamic time warping (DTW) as it got a MCD score closer to that reported in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_mcd(ref_mcep_files, synth_mcep_files, cost_function):\n",
    "    \"\"\"\n",
    "    Calculate the average MCD.\n",
    "    :param ref_mcep_files: list of strings, paths to MCEP target reference files\n",
    "    :param synth_mcep_files: list of strings, paths to MCEP converted synthesised files\n",
    "    :param cost_function: distance metric used\n",
    "    :returns: average MCD, total frames processed\n",
    "    \"\"\"\n",
    "    min_cost_tot = 0.0\n",
    "    frames_tot = 0\n",
    "    \n",
    "    for ref in ref_mcep_files:\n",
    "        for synth in synth_mcep_files:\n",
    "            ref_id = os.path.basename(ref)\n",
    "            synth_id = os.path.basename(synth).split('_',1)[-1]\n",
    "            # if the speaker name is the same and sample id is the same, do MCD\n",
    "            if ref_id == synth_id:\n",
    "                # load MCEP vectors\n",
    "                ref_vec = np.load(ref)\n",
    "                ref_frame_no = len(ref_vec)\n",
    "                synth_vec = np.load(synth)\n",
    "\n",
    "                # dynamic time warping using librosa\n",
    "                min_cost, _ = librosa.sequence.dtw(ref_vec[:, 1:].T, synth_vec[:, 1:].T, \n",
    "                                                   metric=cost_function)               \n",
    "                min_cost_tot += np.mean(min_cost)\n",
    "                frames_tot += ref_frame_no\n",
    "                \n",
    "    mean_mcd = min_cost_tot / frames_tot\n",
    "    \n",
    "    return mean_mcd, frames_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepvoice MCD = 7.780104466000925 dB, calculated over a total of 133460 frames\n",
      "tacotron MCD = 6.416187556584367 dB, calculated over a total of 60339 frames\n"
     ]
    }
   ],
   "source": [
    "deep_origin_refs = glob.glob('./audio_deep/mceps_numpy/audio_origin/*')\n",
    "deep_syn_synths = glob.glob('./audio_deep/mceps_numpy/audio_syn/*')\n",
    "taco_origin_refs = glob.glob('./audio_taco/mceps_numpy/audio_origin/*')\n",
    "taco_syn_synths = glob.glob('./audio_taco/mceps_numpy/audio_syn/*')\n",
    "\n",
    "cost_function = log_spec_dB_dist\n",
    "\n",
    "deep_mcd, deep_tot_frames_used = average_mcd(deep_origin_refs, deep_syn_synths, cost_function)\n",
    "taco_mcd, taco_tot_frames_used = average_mcd(taco_origin_refs, taco_syn_synths, cost_function)\n",
    "\n",
    "\n",
    "print(f'deepvoice MCD = {deep_mcd} dB, calculated over a total of {deep_tot_frames_used} frames')\n",
    "print(f'tacotron MCD = {taco_mcd} dB, calculated over a total of {taco_tot_frames_used} frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
